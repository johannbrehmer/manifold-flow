{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture model on a polynomial surface: result table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all = True\n",
    "n_runs = 5\n",
    "remove_all_results_with_nans = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_filenames = []\n",
    "algo_additionals = []\n",
    "algo_labels = []\n",
    "algo_dividers = []\n",
    "\n",
    "def add_algo(filename, add, label, shortlist=False):\n",
    "    if show_all or shortlist:\n",
    "        algo_filenames.append(filename)\n",
    "        algo_additionals.append(add)\n",
    "        algo_labels.append(label)\n",
    "    \n",
    "    \n",
    "def add_divider():\n",
    "    algo_dividers.append(len(algo_filenames))\n",
    "    \n",
    "\n",
    "add_algo(\"flow\", \"_march\", \"AF\")\n",
    "add_algo(\"pie\", \"_march\", \"PIE\")\n",
    "add_algo(\"mf\", \"_march\", \"FLM--S\")\n",
    "add_algo(\"mf\", \"_alternate_march\", \"FLM--A\")\n",
    "add_algo(\"gamf\", \"_march\", \"FLM--OT\")\n",
    "add_algo(\"gamf\", \"_alternate_march\", \"FLM--OTA\")\n",
    "add_algo(\"emf\", \"_march\", \"FLME--S\")\n",
    "add_algo(\"emf\", \"_alternate_march\", \"FLME--A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(name, shape, numpyfy=True, result_dir=\"../data/results\"):\n",
    "    all_results = []\n",
    "    \n",
    "    for algo_filename, algo_add in zip(algo_filenames, algo_additionals):\n",
    "        algo_results = []\n",
    "            \n",
    "        for run in range(n_runs):\n",
    "            run_str = \"\" if run == 0 else \"_run{}\".format(run)\n",
    "            try:\n",
    "                this_result = np.load(\n",
    "                    \"{}/{}_2_power{}{}_{}.npy\".format(\n",
    "                        result_dir, algo_filename, algo_add, run_str, name\n",
    "                    )\n",
    "                )\n",
    "                if (not numpyfy) or (shape is None) or np.product(this_result.shape) == np.product(shape):\n",
    "                    algo_results.append(this_result.reshape(shape))\n",
    "                else:\n",
    "                    algo_results.append(np.nan*np.ones(shape))\n",
    "                    \n",
    "            except FileNotFoundError as e:\n",
    "                print(e)\n",
    "                if shape is None:\n",
    "                    algo_results.append(None)\n",
    "                else:\n",
    "                    algo_results.append(np.nan*np.ones(shape))\n",
    "            \n",
    "        all_results.append(algo_results)\n",
    "    \n",
    "    if numpyfy:\n",
    "        all_results = np.array(all_results, dtype=np.float)\n",
    "        \n",
    "    return all_results\n",
    "\n",
    "\n",
    "model_gen_x = load(\"samples\", None, numpyfy=False)\n",
    "model_gen_logp = load(\"samples_likelihood\", (10000,))\n",
    "model_gen_distance = load(\"samples_manifold_distance\", (10000,))\n",
    "model_test_logp = load(\"model_log_likelihood_test\", (101, 1000,))\n",
    "model_test_reco_error = load(\"model_reco_error_test\", (1000,))\n",
    "model_ood_logp = load(\"model_log_likelihood_ood\", (101, 1000,))\n",
    "model_ood_reco_error = load(\"model_reco_error_ood\", (1000,))\n",
    "model_posterior_samples = load(\"posterior_samples\", (5000, 1,))\n",
    "model_mmds = load(\"mmd\", (1,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nans(*results, label=None):\n",
    "    passes = all([np.all(np.isfinite(result)) for result in results])\n",
    "    \n",
    "    if passes:\n",
    "        return results\n",
    "    else:\n",
    "        if label is not None:\n",
    "            print(label, \"removed:\", [np.all(np.isfinite(result)) for result in results])\n",
    "        return [np.nan * np.ones_like(result) for result in results]\n",
    "\n",
    "\n",
    "def remove_nans_from_lists(*raws):\n",
    "    # raws[quantity][algo]\n",
    "    n_quantities = len(raws)\n",
    "    n_algos = len(raws[0])\n",
    "    \n",
    "    for raw in raws:\n",
    "        assert len(raw) == n_algos\n",
    "    \n",
    "    cleans = [[[] for _ in range(n_algos)] for _ in range(n_quantities)]\n",
    "    \n",
    "    for i in range(n_algos):\n",
    "        for k in range(n_runs):\n",
    "            clean = remove_nans(*[raw[i][k] for raw in raws], label=\"{}, run {}\".format(algo_labels[i], k))\n",
    "            for j in range(n_quantities):\n",
    "                cleans[j][i].append(clean[j])\n",
    "            \n",
    "    cleans = [np.array(clean) for clean in cleans]\n",
    "    \n",
    "    # cleans[quantity][algo]\n",
    "    return cleans\n",
    "   \n",
    "    \n",
    "if remove_all_results_with_nans:         \n",
    "    raw = [model_gen_logp, model_gen_distance, model_test_logp, model_ood_logp, model_ood_reco_error, model_posterior_samples, model_mmds]\n",
    "    clean = remove_nans_from_lists(*raw)\n",
    "    model_gen_logp, model_gen_distance, model_test_logp, model_ood_logp, model_ood_reco_error, model_posterior_samples, model_mmds = clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_truth(name, samples=True):\n",
    "    if samples:\n",
    "        return np.asarray([\n",
    "            np.load(\"../data/samples/power/{}{}.npy\".format(\n",
    "                name, run_str\n",
    "            ))\n",
    "            for run_str in [\"\"] + [\"_run{}\".format(i) for i in range(1, n_runs)]\n",
    "        ])\n",
    "    else:\n",
    "        return np.asarray([\n",
    "            np.load(\"../data/results/truth_power{}_{}.npy\".format(\n",
    "                run_str, name\n",
    "            ))\n",
    "            for run_str in [\"\"] + [\"_run{}\".format(i) for i in range(1, n_runs)]\n",
    "        ])\n",
    "\n",
    "test_x = load_truth(\"x_test\", True)\n",
    "test_distance = np.zeros((test_x.shape[0], 1))\n",
    "test_logp = load_truth(\"true_log_likelihood_test\", False)\n",
    "true_posterior_samples = load_truth(\"posterior_samples\", False)\n",
    "\n",
    "param_grid = np.linspace(-1, 1, 101)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_observed = 20\n",
    "min_logp = -100.\n",
    "max_distance = 10.\n",
    "\n",
    "model_gen_mean_logp = np.mean(np.clip(model_gen_logp, min_logp, None), axis=2)\n",
    "model_gen_mean_distance = np.mean(np.clip(model_gen_distance, None, max_distance), axis=2)\n",
    "model_observed_nll = -2. * np.sum(model_test_logp[:,:,:,:n_observed], axis=-1)\n",
    "model_test_mean_reco_error = np.mean(np.clip(model_test_reco_error, None, max_distance), axis=2)\n",
    "\n",
    "test_mean_logp = np.mean(np.clip(test_logp, min_logp, None), axis=1)\n",
    "test_mean_distance = np.mean(np.clip(test_distance, None, max_distance), axis=1)\n",
    "true_observed_nll = -2. * np.sum(test_logp[:,:,:n_observed], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_roc_auc(x0, x1):\n",
    "    assert x0.shape == x1.shape\n",
    "    old_shape = x0.shape[:-1]\n",
    "    x0 = x0.reshape(-1, x0.shape[-1])\n",
    "    x1 = x1.reshape(-1, x1.shape[-1])\n",
    "    \n",
    "    aucs = []\n",
    "    for x0_, x1_ in zip(x0, x1):\n",
    "        if not np.all(np.isfinite(np.hstack((x0_, x1_)))):\n",
    "            aucs.append(np.nan)\n",
    "            continue\n",
    "            \n",
    "        auc = roc_auc_score(\n",
    "            np.hstack((np.zeros(x0_.shape[0], dtype=np.int), np.ones(x1_.shape[0], dtype=np.int))),\n",
    "            np.hstack((x0_, x1_)),\n",
    "        )\n",
    "        auc_flipped = roc_auc_score(\n",
    "            np.hstack((np.zeros(x0_.shape[0], dtype=np.int), np.ones(x1_.shape[0], dtype=np.int))),\n",
    "            - np.hstack((x0_, x1_)),\n",
    "        )\n",
    "        aucs.append(max(auc, auc_flipped))\n",
    "        \n",
    "    aucs = np.asarray(aucs)\n",
    "    aucs = aucs.reshape(old_shape)\n",
    "    return aucs\n",
    "\n",
    "\n",
    "model_auc_logp = calculate_roc_auc(model_test_logp[:,:,50,:], model_ood_logp[:,:,50,:])\n",
    "model_auc_err = calculate_roc_auc(model_test_reco_error, model_ood_reco_error)\n",
    "model_auc_use_err = (model_auc_err > model_auc_logp)\n",
    "model_auc = np.maximum(model_auc_err, model_auc_logp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLM--S\n",
      "FLM--A\n",
      "AF\n",
      "FLM--A\n"
     ]
    }
   ],
   "source": [
    "best_mmd = np.nanargmin(np.nanmean(model_mmds, axis=1))\n",
    "print(algo_labels[best_mmd])\n",
    "\n",
    "best_dist = np.nanargmin(np.nanmean(model_gen_mean_distance, axis=1))\n",
    "print(algo_labels[best_dist])\n",
    "\n",
    "best_auc = np.nanargmax(np.nanmean(model_auc, axis=1))\n",
    "print(algo_labels[best_auc])\n",
    "\n",
    "model_test_mean_reco_error_ = np.copy(model_test_mean_reco_error)\n",
    "model_test_mean_reco_error_[model_test_mean_reco_error_ < 1.e-3] = 1000.\n",
    "best_recoerr = np.nanargmin(np.nanmean(model_test_mean_reco_error_, axis=1))\n",
    "print(algo_labels[best_recoerr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(\n",
    "    include_err=False, include_n_runs=False, include_runs=False,\n",
    "    l_label=max([len(l) for l in algo_labels]), l_mean=5, l_err=3,\n",
    "    median=False, latex=False\n",
    "):\n",
    "    # How to format the numbers\n",
    "    l_result = l_mean + int(include_err) * (2 + l_err) + int(include_n_runs) * 4 + int(include_runs) * (3 + 3*l_mean + 2*2)\n",
    "    l_total = l_label + 1 + 4 * (3 + l_result)\n",
    "        \n",
    "    # Divider\n",
    "    empty_result = \"\" if latex else \" \"*(l_result + 1)\n",
    "    col_divider = \"&\" if latex else \"|\"\n",
    "    line_end = r\"\\\\\" if latex else \"\"\n",
    "    block_divider = r\"\\midrule\" if latex else \"-\"*l_total\n",
    "    \n",
    "    def _f(val, best=False):\n",
    "        if not np.any(np.isfinite(val)):\n",
    "            return empty_result\n",
    "        \n",
    "        if median:\n",
    "            result = \"{:>{}.{}f}\".format(np.nanmedian(val), l_mean, l_mean - 2)\n",
    "        else:\n",
    "            result = \"{:>{}.{}f}\".format(np.nanmean(val), l_mean, l_mean - 2)\n",
    "            \n",
    "        if latex and best:\n",
    "            result = r\"\\textbf{\" + result + \"}\"\n",
    "            \n",
    "        if include_err:\n",
    "            err_str = \"({:0>{}d})\".format(int(10**l_err * np.nanstd(val) / np.sum(np.isfinite(val))**0.5), l_err)\n",
    "            if latex:\n",
    "                result += r\"\\,\\textcolor{dark-gray}{\" + err_str + \"}\"\n",
    "            else:\n",
    "                result += err_str\n",
    "            \n",
    "        if include_n_runs:\n",
    "            result += \" [{:1n}]\".format(np.sum(np.isfinite(val)))\n",
    "        if include_runs:\n",
    "            result += \" [{:>{}.{}f}, \".format(float(val[0]), l_mean, l_mean - 2)\n",
    "            result += \"{:>{}.{}f}, \".format(float(val[1]), l_mean, l_mean - 2)\n",
    "            result += \"{:>{}.{}f}, \".format(float(val[2]), l_mean, l_mean - 2)\n",
    "            result += \"{:>{}.{}f}, \".format(float(val[3]), l_mean, l_mean - 2)\n",
    "            result += \"{:>{}.{}f}]\".format(float(val[4]), l_mean, l_mean - 2)\n",
    "            \n",
    "        if (not latex) and best:\n",
    "            result += \"*\"\n",
    "        elif (not latex) and (not best):\n",
    "            result += \" \"\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    # Header\n",
    "    print(\n",
    "        \"{2:<{0}.{0}s} {7} {3:>{1}.{1}s} {7} {4:>{1}.{1}s} {7} {5:>{1}.{1}s} {7} {6:>{1}.{1}s} {8}\".format(\n",
    "            l_label, l_result, \"\", \"Man. dist.\", \"Reco err.\", \"Post. MMD\", \"OOD AUC\", col_divider, line_end\n",
    "        )\n",
    "    )\n",
    "    print(block_divider)\n",
    "\n",
    "    # Iterate over methods\n",
    "    for i, (label, dist, mmd, auc, recoerr) in enumerate(zip(\n",
    "        algo_labels, model_gen_mean_distance, model_mmds, model_auc, model_test_mean_reco_error\n",
    "    )):\n",
    "        # Divider\n",
    "        if i in algo_dividers and show_all:\n",
    "            print(block_divider)\n",
    "            \n",
    "        # Print results\n",
    "        print(\"{1:<{0}.{0}s} {6} {2}{6} {5}{6} {4}{6} {3} {7}\".format(\n",
    "            l_label, label,\n",
    "            _f(dist, i==best_dist), _f(auc, i==best_auc),  _f(mmd, i==best_mmd), _f(recoerr, i==best_recoerr),\n",
    "            col_divider, line_end\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         |                  Man. dist. |                   Reco err. |                   Post. MMD |                     OOD AUC \n",
      "---------------------------------------------------------------------------------------------------------------------------------\n",
      "AF       | 0.005 [0.009, 0.003, 0.005, 0.003, 0.005] | 0.000 [0.000, 0.000, 0.000, 0.000, 0.000] | 0.158 [0.476, 0.026, 0.028, 0.190, 0.071] | 0.989 [0.990, 0.991, 0.991, 0.987, 0.988]* \n",
      "PIE      | 0.020 [0.006, 0.065, 0.005, 0.003, 0.020] | 1.205 [1.699, 0.753, 1.608, 1.253, 0.712] | 0.070 [0.121, 0.014, 0.075, 0.129, 0.010] | 0.966 [0.985, 0.972, 0.964, 0.973, 0.937]  \n",
      "FLM--S   | 0.059 [0.269, 0.004, 0.014, 0.003, 0.006] | 0.052 [0.225, 0.011, 0.018, 0.002, 0.006] | 0.042 [0.006, 0.149, 0.031, 0.001, 0.026]*| 0.887 [0.561, 0.978, 0.942, 0.980, 0.974]  \n",
      "FLM--A   | 0.003 [0.002, 0.002, 0.002, 0.004, 0.003]*| 0.005 [0.002, 0.001, 0.008, 0.003, 0.009]*| 0.066 [0.001, 0.020, 0.176, 0.011, 0.124] | 0.986 [0.987, 0.992, 0.984, 0.986, 0.983]  \n",
      "FLM--OT  | 0.093 [0.086, 0.139, 0.089, 0.096, 0.054] | 0.514 [0.649, 0.433, 0.145, 1.061, 0.282] | 0.205 [0.553, 0.151, 0.126, 0.134, 0.062] | 0.648 [0.607, 0.592, 0.703, 0.647, 0.690]  \n",
      "FLM--OTA | 0.211 [0.413, 0.142, 0.085, 0.309, 0.107] | 1.060 [1.952, 1.206, 0.248, 1.121, 0.770] | 0.150 [0.515, 0.113, 0.035, 0.035, 0.051] | 0.584 [0.549, 0.584, 0.652, 0.534, 0.599]  \n",
      "FLME--S  | 0.074 [0.004, 0.351, 0.003, 0.005, 0.005] | 0.127 [0.006, 0.609, 0.004, 0.012, 0.005] | 0.066 [0.007, 0.122, 0.032, 0.033, 0.137] | 0.886 [0.975, 0.524, 0.982, 0.971, 0.978]  \n",
      "FLME--A  | 0.647 [0.726, 0.984, 0.696, 0.389, 0.440] | 1.418 [1.230, 1.495, 2.233, 1.327, 0.804] | 0.049 [0.010, 0.001, 0.082, 0.027, 0.124] | 0.547 [0.579, 0.534, 0.528, 0.553, 0.542]  \n"
     ]
    }
   ],
   "source": [
    "print_results(include_runs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         | Man. dist. |  Reco err. |  Post. MMD |    OOD AUC \n",
      "-------------------------------------------------------------\n",
      "AF       | 0.005(000) | 0.000(000) | 0.158(075) | 0.989(000)* \n",
      "PIE      | 0.020(010) | 1.205(185) | 0.070(022) | 0.966(007)  \n",
      "FLM--S   | 0.059(047) | 0.052(038) | 0.042(024)*| 0.887(073)  \n",
      "FLM--A   | 0.003(000)*| 0.005(001)*| 0.066(031) | 0.986(001)  \n",
      "FLM--OT  | 0.093(012) | 0.514(143) | 0.205(079) | 0.648(019)  \n",
      "FLM--OTA | 0.211(057) | 1.060(250) | 0.150(082) | 0.584(018)  \n",
      "FLME--S  | 0.074(062) | 0.127(107) | 0.066(023) | 0.886(080)  \n",
      "FLME--A  | 0.647(096) | 1.418(208) | 0.049(020) | 0.547(008)  \n"
     ]
    }
   ],
   "source": [
    "print_results(include_err=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         & Man. dist. &  Reco err. &  Post. MMD &    OOD AUC \\\\\n",
      "\\midrule\n",
      "AF       & 0.005\\,\\textcolor{dark-gray}{(000)}& 0.000\\,\\textcolor{dark-gray}{(000)}& 0.158\\,\\textcolor{dark-gray}{(075)}& \\textbf{0.989}\\,\\textcolor{dark-gray}{(000)} \\\\\n",
      "PIE      & 0.020\\,\\textcolor{dark-gray}{(010)}& 1.205\\,\\textcolor{dark-gray}{(185)}& 0.070\\,\\textcolor{dark-gray}{(022)}& 0.966\\,\\textcolor{dark-gray}{(007)} \\\\\n",
      "FLM--S   & 0.059\\,\\textcolor{dark-gray}{(047)}& 0.052\\,\\textcolor{dark-gray}{(038)}& \\textbf{0.042}\\,\\textcolor{dark-gray}{(024)}& 0.887\\,\\textcolor{dark-gray}{(073)} \\\\\n",
      "FLM--A   & \\textbf{0.003}\\,\\textcolor{dark-gray}{(000)}& \\textbf{0.005}\\,\\textcolor{dark-gray}{(001)}& 0.066\\,\\textcolor{dark-gray}{(031)}& 0.986\\,\\textcolor{dark-gray}{(001)} \\\\\n",
      "FLM--OT  & 0.093\\,\\textcolor{dark-gray}{(012)}& 0.514\\,\\textcolor{dark-gray}{(143)}& 0.205\\,\\textcolor{dark-gray}{(079)}& 0.648\\,\\textcolor{dark-gray}{(019)} \\\\\n",
      "FLM--OTA & 0.211\\,\\textcolor{dark-gray}{(057)}& 1.060\\,\\textcolor{dark-gray}{(250)}& 0.150\\,\\textcolor{dark-gray}{(082)}& 0.584\\,\\textcolor{dark-gray}{(018)} \\\\\n",
      "FLME--S  & 0.074\\,\\textcolor{dark-gray}{(062)}& 0.127\\,\\textcolor{dark-gray}{(107)}& 0.066\\,\\textcolor{dark-gray}{(023)}& 0.886\\,\\textcolor{dark-gray}{(080)} \\\\\n",
      "FLME--A  & 0.647\\,\\textcolor{dark-gray}{(096)}& 1.418\\,\\textcolor{dark-gray}{(208)}& 0.049\\,\\textcolor{dark-gray}{(020)}& 0.547\\,\\textcolor{dark-gray}{(008)} \\\\\n"
     ]
    }
   ],
   "source": [
    "print_results(include_err=True, latex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ML)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
